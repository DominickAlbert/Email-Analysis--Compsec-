{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1643ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f35324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"CEAS_08.csv\")\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95154faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39154 entries, 0 to 39153\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sender    39154 non-null  object\n",
      " 1   receiver  38692 non-null  object\n",
      " 2   date      39154 non-null  object\n",
      " 3   subject   39126 non-null  object\n",
      " 4   body      39154 non-null  object\n",
      " 5   label     39154 non-null  int64 \n",
      " 6   urls      39154 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3f30b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c739e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", str(text))  \n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-]\", \"\", text)  # Retain hyphens\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text, flags=re.UNICODE)  \n",
    "    text = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "    return \" \".join(text).strip()    \n",
    "\n",
    "\n",
    "df[\"clean_subject\"] = df[\"subject\"].apply(clean_text)\n",
    "df[\"clean_body\"] = df[\"body\"].apply(clean_text)\n",
    "\n",
    "# Extract sender domain\n",
    "df[\"sender_domain\"] = df[\"sender\"].apply(lambda x: x.split(\"@\")[-1] if pd.notnull(x) else \"\")\n",
    "df['sender_domain'] = df['sender_domain'].str[:-1]\n",
    "\n",
    "df[\"receiver_domain\"] = df[\"receiver\"].apply(lambda x: x.split(\"@\")[-1] if pd.notnull(x) else \"\")\n",
    "# df['receiver_domain'] = df['receiver_domain'].str[:-1]\n",
    "df['receiver_domain'] = df['receiver_domain'].apply(lambda x: x[:-1] if x.endswith(\">\") else x)\n",
    "\n",
    "# Parse date (handle inconsistent formats)\n",
    "df[\"date\"] = df[\"date\"].apply(lambda x: pd.to_datetime(x, errors=\"coerce\",utc = True))\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.dayofweek \n",
    "df['hour_normalized'] = df['hour'] / 23.0\n",
    "\n",
    "df = df.dropna(subset=[\"label\", \"clean_subject\", \"clean_body\",\"receiver\",\"subject\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fc6804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    21812\n",
       "0    16842\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec730b7",
   "metadata": {},
   "source": [
    "# Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b69979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    16842\n",
       "0    16842\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the majority and minority classes\n",
    "majority_class = df[df['label'] == 1]\n",
    "minority_class = df[df['label'] == 0]\n",
    "\n",
    "# Randomly sample from the majority class to match the size of the minority class\n",
    "balanced_majority_class = majority_class.sample(len(minority_class), random_state=42)\n",
    "\n",
    "# Combine the balanced majority class with the minority class\n",
    "df_balanced = pd.concat([balanced_majority_class, minority_class])\n",
    "\n",
    "# Shuffle the resulting dataframe\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_balanced['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ef8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced.drop(columns=[\"date\", \"sender\", \"receiver\", \"subject\", \"body\",\"hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dea0b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33684 entries, 0 to 33683\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   label            33684 non-null  int64  \n",
      " 1   urls             33684 non-null  int64  \n",
      " 2   clean_subject    33684 non-null  object \n",
      " 3   clean_body       33684 non-null  object \n",
      " 4   sender_domain    33684 non-null  object \n",
      " 5   receiver_domain  33684 non-null  object \n",
      " 6   day_of_week      33684 non-null  float64\n",
      " 7   hour_normalized  33684 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(4)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_balanced.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6539f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced.to_csv(\"CEAS_08_cleaned_balanced.csv\", index=False)\n",
    "df_balanced = pd.read_csv(\"CEAS_08_cleaned_balanced.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a5569",
   "metadata": {},
   "source": [
    "# Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcd693e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rel ys iab vk le dru yar gs xb tore high qua lzp lity lh di chk cinec wkn li qsj ck na'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_balanced\n",
    "df_balanced['clean_body'].iloc[33682]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde9e1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>clean_subject</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>sender_domain</th>\n",
       "      <th>receiver_domain</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cnncom daily top 10</td>\n",
       "      <td>daily top 10 cnncom top videos stories aug 1 2...</td>\n",
       "      <td>2905.dk</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>canadian chemist trust j</td>\n",
       "      <td>find love stick gain click url ydrvl5a</td>\n",
       "      <td>tvgam.org.uk</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ilug via epia server</td>\n",
       "      <td>newer generations via epia boxes withe c32 pro...</td>\n",
       "      <td>lincor.com</td>\n",
       "      <td>birdsnest.maths.tcd.ie</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>opensuse amavisd warning failure</td>\n",
       "      <td>patrick shanahan wrote hylton conacher zr1hpc ...</td>\n",
       "      <td>conacher.co.za</td>\n",
       "      <td>opensuse.org</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cnncom daily top 10</td>\n",
       "      <td>daily top 10 cnncom top videos stories aug 1 2...</td>\n",
       "      <td>duluth.com</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  urls                     clean_subject  \\\n",
       "0      1     1               cnncom daily top 10   \n",
       "1      1     0          canadian chemist trust j   \n",
       "2      0     0              ilug via epia server   \n",
       "3      0     0  opensuse amavisd warning failure   \n",
       "4      1     1               cnncom daily top 10   \n",
       "\n",
       "                                          clean_body   sender_domain  \\\n",
       "0  daily top 10 cnncom top videos stories aug 1 2...         2905.dk   \n",
       "1             find love stick gain click url ydrvl5a    tvgam.org.uk   \n",
       "2  newer generations via epia boxes withe c32 pro...      lincor.com   \n",
       "3  patrick shanahan wrote hylton conacher zr1hpc ...  conacher.co.za   \n",
       "4  daily top 10 cnncom top videos stories aug 1 2...      duluth.com   \n",
       "\n",
       "          receiver_domain  day_of_week  hour_normalized  \n",
       "0   gvc.ceas-challenge.cc          2.0         0.869565  \n",
       "1   gvc.ceas-challenge.cc          2.0         0.913043  \n",
       "2  birdsnest.maths.tcd.ie          2.0         0.043478  \n",
       "3            opensuse.org          2.0         0.652174  \n",
       "4   gvc.ceas-challenge.cc          3.0         0.434783  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4affcb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4da73498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 26947 entries, 20762 to 14000\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   urls             26947 non-null  int64  \n",
      " 1   clean_subject    26462 non-null  object \n",
      " 2   clean_body       26947 non-null  object \n",
      " 3   sender_domain    26947 non-null  object \n",
      " 4   receiver_domain  26947 non-null  object \n",
      " 5   day_of_week      26947 non-null  float64\n",
      " 6   hour_normalized  26947 non-null  float64\n",
      " 7   subjectAndBody   26462 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m     11\u001b[0m X_train\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m---> 12\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubjectAndBody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubjectAndBody\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[0;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# combine the subject and body for tfidf\n",
    "df_balanced['subjectAndBody'] = df_balanced['clean_subject'] + ' ' + df_balanced['clean_body']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced.drop(columns='label'), df_balanced['label'], test_size=0.2, random_state=50\n",
    ")\n",
    "\n",
    "\n",
    "# vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train.info()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['subjectAndBody'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['subjectAndBody'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b673e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5288432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f6bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3386\n",
      "           1       1.00      0.96      0.98      3351\n",
      "\n",
      "    accuracy                           0.98      6737\n",
      "   macro avg       0.98      0.98      0.98      6737\n",
      "weighted avg       0.98      0.98      0.98      6737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040b7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "      <th>clean_subject</th>\n",
       "      <th>clean_body</th>\n",
       "      <th>sender_domain</th>\n",
       "      <th>receiver_domain</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cnncom daily top 10</td>\n",
       "      <td>daily top 10 cnncom top videos stories aug 1 2...</td>\n",
       "      <td>2905.dk</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>canadian chemist trust j</td>\n",
       "      <td>find love stick gain click url ydrvl5a</td>\n",
       "      <td>tvgam.org.uk</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ilug via epia server</td>\n",
       "      <td>newer generations via epia boxes withe c32 pro...</td>\n",
       "      <td>lincor.com</td>\n",
       "      <td>birdsnest.maths.tcd.ie</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>opensuse amavisd warning failure</td>\n",
       "      <td>patrick shanahan wrote hylton conacher zr1hpc ...</td>\n",
       "      <td>conacher.co.za</td>\n",
       "      <td>opensuse.org</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cnncom daily top 10</td>\n",
       "      <td>daily top 10 cnncom top videos stories aug 1 2...</td>\n",
       "      <td>duluth.com</td>\n",
       "      <td>gvc.ceas-challenge.cc</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  urls                     clean_subject  \\\n",
       "0      1     1               cnncom daily top 10   \n",
       "1      1     0          canadian chemist trust j   \n",
       "2      0     0              ilug via epia server   \n",
       "3      0     0  opensuse amavisd warning failure   \n",
       "4      1     1               cnncom daily top 10   \n",
       "\n",
       "                                          clean_body   sender_domain  \\\n",
       "0  daily top 10 cnncom top videos stories aug 1 2...         2905.dk   \n",
       "1             find love stick gain click url ydrvl5a    tvgam.org.uk   \n",
       "2  newer generations via epia boxes withe c32 pro...      lincor.com   \n",
       "3  patrick shanahan wrote hylton conacher zr1hpc ...  conacher.co.za   \n",
       "4  daily top 10 cnncom top videos stories aug 1 2...      duluth.com   \n",
       "\n",
       "          receiver_domain  day_of_week  hour_normalized  \n",
       "0   gvc.ceas-challenge.cc          2.0         0.869565  \n",
       "1   gvc.ceas-challenge.cc          2.0         0.913043  \n",
       "2  birdsnest.maths.tcd.ie          2.0         0.043478  \n",
       "3            opensuse.org          2.0         0.652174  \n",
       "4   gvc.ceas-challenge.cc          3.0         0.434783  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20285104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_phishing_seriousness(\n",
    "    clean_subject, \n",
    "    clean_body, \n",
    "    sender_domain, \n",
    "    receiver_domain, \n",
    "    day_of_week, \n",
    "    hour_normalized, \n",
    "    urls\n",
    "):\n",
    "    score = 0.0\n",
    "    \n",
    "    # 1. Check for URL presence (strong indicator)\n",
    "    if urls == 1:\n",
    "        score += 0.35\n",
    "    \n",
    "    # 2. Sender domain analysis\n",
    "    trusted_domains = {'gmail.com', 'outlook.com', 'yahoo.com', 'hotmail.com', 'protonmail.com',\n",
    "    'icloud.com', 'aol.com', 'zoho.com', 'gmx.com', 'mail.com', 'tutanota.com',\n",
    "    'fastmail.com', 'hushmail.com', 'runbox.com', 'posteo.de', 'disroot.org'\n",
    "    }\n",
    "    if sender_domain not in trusted_domains:\n",
    "        score += 0.25\n",
    "    \n",
    "    # 3. Text analysis for phishing keywords\n",
    "    phishing_keywords = {\n",
    "    'password', 'urgent', 'verify', 'account', 'login', 'bank', 'security', \n",
    "    'suspended', 'confirm', 'fraud', 'update', 'alert', 'compromised', \n",
    "    'immediately', 'limited', 'action', 'required', 'personal', 'information',\n",
    "    'click', 'link', 'attachment', 'unauthorized', 'activity', 'locked', \n",
    "    'expired', 'reactivate', 'invoice', 'payment', 'refund', 'transaction'\n",
    "    }\n",
    "    text = (clean_subject + ' ' + clean_body).lower()\n",
    "    keyword_hits = sum(1 for word in phishing_keywords if word in text)\n",
    "    score += min(keyword_hits * 0.07, 0.3)  # Max 0.3 for text content\n",
    "    \n",
    "    # 4. Timing analysis\n",
    "    # Weekend check (day 6=Saturday, 7=Sunday)\n",
    "    if day_of_week in {6, 7}:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Unusual hours (before 6 AM or after 8 PM)\n",
    "    if hour_normalized < (6/24) or hour_normalized > (20/24):\n",
    "        score += 0.15\n",
    "    \n",
    "    # 5. Domain mismatch check\n",
    "    if sender_domain != receiver_domain:\n",
    "        score += 0.15\n",
    "    \n",
    "    # 6. Check for suspicious sender domain patterns\n",
    "    if any(c in sender_domain for c in ['-', '0', '1', '2', '3']):\n",
    "        score += 0.1\n",
    "    \n",
    "    # Ensure score is within [0, 1]\n",
    "    return max(0.0, min(score, 1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
